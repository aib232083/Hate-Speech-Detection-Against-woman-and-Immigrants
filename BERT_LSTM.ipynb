{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8310229,"sourceType":"datasetVersion","datasetId":4936533}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\n#import os\nimport emoji as emoji\nimport re\nimport string\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom transformers import AutoModel\nfrom transformers import BertModel, BertTokenizer\nimport nltk\nfrom nltk.tokenize import TweetTokenizer\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler","metadata":{"_uuid":"463fbcb0-6e1f-4082-8b3f-616d84ba7a18","_cell_guid":"f313091b-0560-46f9-86fc-1581f5055e54","collapsed":false,"execution":{"iopub.status.busy":"2024-05-04T09:27:06.226467Z","iopub.execute_input":"2024-05-04T09:27:06.226822Z","iopub.status.idle":"2024-05-04T09:27:06.233024Z","shell.execute_reply.started":"2024-05-04T09:27:06.226795Z","shell.execute_reply":"2024-05-04T09:27:06.232128Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ntraindf = pd.read_csv('/kaggle/input/nlpdataset/Book5.csv', encoding='latin1')\nprint(traindf.shape[0])\ntraindf.head(5)","metadata":{"_uuid":"c600e6c8-e1a2-42ba-b516-62de2f1af92d","_cell_guid":"1e458bb7-e03f-428e-9fa0-61215431184e","collapsed":false,"execution":{"iopub.status.busy":"2024-05-04T09:27:06.234870Z","iopub.execute_input":"2024-05-04T09:27:06.235194Z","iopub.status.idle":"2024-05-04T09:27:06.340712Z","shell.execute_reply.started":"2024-05-04T09:27:06.235164Z","shell.execute_reply":"2024-05-04T09:27:06.339770Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.nn.utils.rnn import pack_padded_sequence\n\nclass BERT_LSTM_Model(nn.Module):\n\n  def __init__(self, bert, n_class):\n    dropout_rate = 0.2\n    lstm_hidden_size = None\n\n    super(BERT_LSTM_Model, self).__init__()\n    self.bert = bert\n\n    if not lstm_hidden_size:\n      self.lstm_hidden_size = self.bert.config.hidden_size\n    else:\n      self.lstm_hidden_size = lstm_hidden_size\n    self.n_class = n_class\n    self.dropout_rate = dropout_rate\n    self.lstm = nn.LSTM(self.bert.config.hidden_size, self.lstm_hidden_size, bidirectional=True)\n    self.hidden_to_softmax = nn.Linear(self.lstm_hidden_size * 2, n_class, bias=True)\n    self.dropout = nn.Dropout(p=self.dropout_rate)\n    self.softmax = nn.LogSoftmax(dim=1)\n\n  def forward(self, sent_id, mask):\n    encoded_layers = self.bert(sent_id, attention_mask=mask, output_hidden_states=True)[2] #,output_all_encoded_layers=False)   #output_hidden_states output_hidden_states=True\n    bert_hidden_layer = encoded_layers[12]\n    bert_hidden_layer = bert_hidden_layer.permute(1, 0, 2)   #permute rotates the tensor. if tensor.shape = 3,4,5  tensor.permute(1,0,2), then tensor,shape= 4,3,5  (batch_size, sequence_length, hidden_size)\n\n    sents_lengths = [36 for i in range(len(sent_id))]\n    enc_hiddens, (last_hidden, last_cell) = self.lstm(pack_padded_sequence(bert_hidden_layer, sents_lengths, enforce_sorted=False)) #enforce_sorted=False  #pack_padded_sequence(data and batch_sizes\n    output_hidden = torch.cat((last_hidden[0], last_hidden[1]), dim=1)  # (batch_size, 2*hidden_size)\n    output_hidden = self.dropout(output_hidden)\n    pre_softmax = self.hidden_to_softmax(output_hidden)\n\n    return self.softmax(pre_softmax)\n  \n\n# class BERT_LSTM_Model(nn.Module):\n#     def __init__(self, bert, n_class):\n#         super(BERT_LSTM_Model, self).__init__()\n#         self.bert = bert\n#         self.lstm = nn.LSTM(self.bert.config.hidden_size, self.bert.config.hidden_size, bidirectional=True)\n#         self.hidden_to_softmax = nn.Linear(self.bert.config.hidden_size * 2, n_class, bias=True)\n#         self.dropout = nn.Dropout(p=0.1)\n#         self.softmax = nn.LogSoftmax(dim=1)\n\n#     def forward(self, sent_id, mask):\n#         outputs = self.bert(sent_id, attention_mask=mask)\n#         bert_hidden_layer = outputs.last_hidden_state\n#         bert_hidden_layer = bert_hidden_layer.permute(1, 0, 2)\n#         enc_hiddens, (last_hidden, last_cell) = self.lstm(bert_hidden_layer)\n#         output_hidden = torch.cat((last_hidden[0], last_hidden[1]), dim=1)\n#         output_hidden = self.dropout(output_hidden)\n#         pre_softmax = self.hidden_to_softmax(output_hidden)\n#         return self.softmax(pre_softmax)","metadata":{"_uuid":"fcfab0a4-5ae8-4fb5-aa7a-4dc57b779d3b","_cell_guid":"3da111cd-b9b6-42d0-af78-4d3d89decb83","collapsed":false,"execution":{"iopub.status.busy":"2024-05-04T09:27:06.341983Z","iopub.execute_input":"2024-05-04T09:27:06.342267Z","iopub.status.idle":"2024-05-04T09:27:06.354105Z","shell.execute_reply.started":"2024-05-04T09:27:06.342242Z","shell.execute_reply":"2024-05-04T09:27:06.353087Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_dataset(ty):\n    \n    train_labels = traindf[ty]\n\n    return traindf['text'].tolist(), train_labels\n\n\ndef tokenize(tweet):\n    # instantiate the tokenizer class\n    tokenizer = TweetTokenizer(preserve_case=False, \n                              strip_handles=True,\n                              reduce_len=True)\n\n    # tokenize the tweets\n    tweet_tokens = tokenizer.tokenize(tweet)\n\n    tweets_clean = []\n    for word in tweet_tokens: # Go through every word in your tokens list\n        if word not in string.punctuation:  # remove punctuation\n            tweets_clean.append(word)\n    result = tweets_clean\n    return \" \".join(result)\n\nemoticons = [':-)', ':)', '(:', '(-:', ':))', '((:', ':-D', ':D', 'X-D', 'XD', 'xD', 'xD', '<3', '</3', ':\\*',\n                 ';-)',\n                 ';)', ';-D', ';D', '(;', '(-;', ':-(', ':(', '(:', '(-:', ':,(', ':\\'(', ':\"(', ':((', ':D', '=D',\n                 '=)',\n                 '(=', '=(', ')=', '=-O', 'O-=', ':o', 'o:', 'O:', 'O:', ':-o', 'o-:', ':P', ':p', ':S', ':s', ':@',\n                 ':>',\n                 ':<', '^_^', '^.^', '>.>', 'T_T', 'T-T', '-.-', '*.*', '~.~', ':*', ':-*', 'xP', 'XP', 'XP', 'Xp',\n                 ':-|',\n                 ':->', ':-<', '$_$', '8-)', ':-P', ':-p', '=P', '=p', ':*)', '*-*', 'B-)', 'O.o', 'X-(', ')-X']\n\ndef preprocess(tweet):\n    result = tweet.replace('rt @','@')\n    result = result.replace('@','<user> @')\n    # it will remove hyperlinks\n    result = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '<url>', result)\n\n    # it will remove hashtags. We have to be careful here not to remove \n    # the whole hashtag because text of hashtags contains huge information. \n    # only removing the hash # sign from the word\n    result = re.sub(r'#', '<hashtag>', result)\n\n    # Replace multiple dots with space\n    result = re.sub('\\.\\.+', ' ', result) \n\n\n\n    for char in result:\n        if emoji.is_emoji(char):\n            result = result.replace(char, \"<emoticon >\")\n    for emo in emoticons:\n        result = result.replace(emo, \"<emoticon >\")\n\n    result = tokenize(result)\n    # it will remove single numeric terms in the tweet. \n    result = re.sub(r'[0-9]+', '<number>', result)\n    result = re.sub(r'<number>\\s?st', '<number>', result)\n    result = re.sub(r'<number>\\s?nd', '<number>', result)\n    result = re.sub(r'<number>\\s?rd', '<number>', result)\n    result = re.sub(r'<number>\\s?th', '<number>', result)\n\n    return result\n\ndef pre_process_dataset(values):\n    new_values = list()\n    for value in values:\n        new_values.append(preprocess(value.lower()))\n#     print(values[:5])\n#     print(new_values[:5])\n    return new_values","metadata":{"_uuid":"d88003f8-f19c-4576-9307-6e112be8f9d6","_cell_guid":"b29fead4-f65d-4308-ac7d-60edfc350d57","collapsed":false,"execution":{"iopub.status.busy":"2024-05-04T09:27:06.355932Z","iopub.execute_input":"2024-05-04T09:27:06.356223Z","iopub.status.idle":"2024-05-04T09:27:06.370479Z","shell.execute_reply.started":"2024-05-04T09:27:06.356198Z","shell.execute_reply":"2024-05-04T09:27:06.369652Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_process(data, labels):\n    input_ids = []\n    attention_masks = []\n    bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n    for sentence in data:\n        bert_inp = bert_tokenizer.__call__(sentence, max_length=36,\n                                           padding='max_length', pad_to_max_length=True,\n                                           truncation=True, return_token_type_ids=False)\n\n        input_ids.append(bert_inp['input_ids'])\n        attention_masks.append(bert_inp['attention_mask'])\n\n    input_ids = np.asarray(input_ids)\n    attention_masks = np.array(attention_masks)\n    labels = np.array(labels)\n    return input_ids, attention_masks, labels","metadata":{"_uuid":"cbbd0e39-5abb-4694-af3d-827328733f2a","_cell_guid":"9e55953c-80e4-4524-af82-0e1f6003fed2","collapsed":false,"execution":{"iopub.status.busy":"2024-05-04T09:27:06.371393Z","iopub.execute_input":"2024-05-04T09:27:06.371657Z","iopub.status.idle":"2024-05-04T09:27:06.385682Z","shell.execute_reply.started":"2024-05-04T09:27:06.371635Z","shell.execute_reply":"2024-05-04T09:27:06.384892Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_and_process(ty):\n\n    data, labels = read_dataset(ty)\n#     num_of_labels = len(labels.unique())\n    input_ids, attention_masks, labels = data_process(pre_process_dataset(data), labels)\n\n    return input_ids, attention_masks, labels","metadata":{"_uuid":"045c933d-9fd1-43f4-9905-ca637bf3a4bd","_cell_guid":"c7e1f313-0914-49b7-8c12-84480624684b","collapsed":false,"execution":{"iopub.status.busy":"2024-05-04T09:27:06.386749Z","iopub.execute_input":"2024-05-04T09:27:06.387023Z","iopub.status.idle":"2024-05-04T09:27:06.398051Z","shell.execute_reply.started":"2024-05-04T09:27:06.387000Z","shell.execute_reply":"2024-05-04T09:27:06.397162Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to train the model\ndef train(model,optimizer,train_dataloader,batch_size):\n    model.train()\n\n    total_loss, total_accuracy = 0, 0\n\n    # empty list to save model predictions\n    total_preds = []\n\n    # iterate over batches\n    total = len(train_dataloader)\n    for i, batch in enumerate(train_dataloader):\n\n        step = i+1\n        percent = \"{0:.2f}\".format(100 * (step / float(total)))\n        lossp = \"{0:.2f}\".format(total_loss/(total*batch_size))\n        filledLength = int(100 * step // total)\n        bar = '█' * filledLength + '>'  *(filledLength < 100) + '.' * (99 - filledLength)\n        print(f'\\rBatch {step}/{total} |{bar}| {percent}% complete, loss={lossp}, accuracy={total_accuracy}', end='')\n\n        # push the batch to gpu\n        batch = [r.to(device) for r in batch]\n        sent_id, mask, labels = batch\n        del batch\n        gc.collect()\n        torch.cuda.empty_cache()\n        # clear previously calculated gradients\n        model.zero_grad()\n\n        preds = model(sent_id, mask)\n\n        # compute the loss between actual and predicted values\n        labels_tensor = labels.clone().detach().to(device).long()\n        loss = cross_entropy(preds, labels_tensor)\n\n        # add on to the total loss\n        total_loss += float(loss.item())\n\n        # backward pass to calculate the gradients\n        loss.backward()\n\n        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # update parameters\n        optimizer.step()\n\n        total_preds.append(preds.detach().cpu().numpy())\n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    # compute the training loss of the epoch\n    avg_loss = total_loss / (len(train_dataloader)*batch_size)\n\n    total_preds = np.concatenate(total_preds, axis=0)\n\n    # returns the loss and predictions\n    return avg_loss, total_preds","metadata":{"_uuid":"66e976cf-c3d3-42b3-9467-433c14b771a8","_cell_guid":"bee3370f-bfdd-4dd0-80c4-750384f99c4e","collapsed":false,"execution":{"iopub.status.busy":"2024-05-04T09:27:06.469699Z","iopub.execute_input":"2024-05-04T09:27:06.469963Z","iopub.status.idle":"2024-05-04T09:27:06.481578Z","shell.execute_reply.started":"2024-05-04T09:27:06.469940Z","shell.execute_reply":"2024-05-04T09:27:06.480736Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function for evaluating the model\ndef evaluate(model,val_dataloader,batch_size):\n    print(\"\\n\\nEvaluating...\")\n\n    # deactivate dropout layers\n    model.eval()\n\n    total_loss, total_accuracy = 0, 0\n\n    # empty list to save the model predictions\n    total_preds = []\n\n    # iterate over batches\n    total = len(val_dataloader)\n    for i, batch in enumerate(val_dataloader):\n        \n        step = i+1\n        percent = \"{0:.2f}\".format(100 * (step / float(total)))\n        lossp = \"{0:.2f}\".format(total_loss/(total*batch_size))\n        filledLength = int(100 * step // total)\n        bar = '█' * filledLength + '>' * (filledLength < 100) + '.' * (99 - filledLength)\n        print(f'\\rBatch {step}/{total} |{bar}| {percent}% complete, loss={lossp}, accuracy={total_accuracy}', end='')\n\n        # push the batch to gpu\n        batch = [t.to(device) for t in batch]\n\n        sent_id, mask, labels = batch\n        del batch\n        gc.collect()\n        torch.cuda.empty_cache()\n        # deactivate autograd\n        with torch.no_grad():\n\n            # model predictions\n            preds = model(sent_id, mask)\n\n\n            labels_tensor = labels.clone().detach().to(device).long()\n            loss = cross_entropy(preds, labels_tensor)\n\n            total_loss += float(loss.item())\n            #preds = preds.detach().cpu().numpy()\n\n            #total_preds.append(preds)\n            total_preds.append(preds.detach().cpu().numpy())\n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    # compute the validation loss of the epoch\n    avg_loss = total_loss / (len(val_dataloader)*batch_size)\n\n    # reshape the predictions in form of (number of samples, no. of classes)\n    total_preds = np.concatenate(total_preds, axis=0)\n\n    return avg_loss, total_preds","metadata":{"_uuid":"f08fca61-92bb-4d6f-b153-d4c0afc18082","_cell_guid":"fe2312ac-3c28-484f-ae2f-79ddad3d0a7c","collapsed":false,"execution":{"iopub.status.busy":"2024-05-04T09:27:06.483356Z","iopub.execute_input":"2024-05-04T09:27:06.483660Z","iopub.status.idle":"2024-05-04T09:27:06.496017Z","shell.execute_reply.started":"2024-05-04T09:27:06.483636Z","shell.execute_reply":"2024-05-04T09:27:06.495215Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify the GPU\n# Setting up the device for GPU usage\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(device)\n\n# loss function\n#cross_entropy = nn.NLLLoss(weight=weights)\ncross_entropy = nn.NLLLoss()","metadata":{"_uuid":"c30ac5e5-903d-4637-b574-68967fa2070b","_cell_guid":"115a9d41-2e43-4c99-a130-741b04e7c8ea","collapsed":false,"execution":{"iopub.status.busy":"2024-05-04T09:27:06.496980Z","iopub.execute_input":"2024-05-04T09:27:06.497538Z","iopub.status.idle":"2024-05-04T09:27:06.508489Z","shell.execute_reply.started":"2024-05-04T09:27:06.497495Z","shell.execute_reply":"2024-05-04T09:27:06.507727Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef modelss(ty):\n\n    # Load Dataset\n    input_ids, attention_masks, labels = load_and_process(ty)\n    df = pd.DataFrame(list(zip(input_ids, attention_masks)), columns=['input_ids', 'attention_masks'])\n\n    # Class distribution\n    train_text, val_text, train_labels, val_labels = train_test_split(df, labels, test_size=0.1, random_state=42)\n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    train_count = len(train_labels)\n    val_count = len(val_labels)\n\n\n    # for train set\n    train_seq = torch.tensor(train_text['input_ids'].tolist())\n    train_mask = torch.tensor(train_text['attention_masks'].tolist())\n    train_y = torch.tensor(train_labels.astype(int))\n\n    # for validation set\n    val_seq = torch.tensor(val_text['input_ids'].tolist())\n    val_mask = torch.tensor(val_text['attention_masks'].tolist())\n    val_y = torch.tensor(val_labels.astype(int))\n\n\n\n    # define a batch size\n    batch_size = 32\n    learning_rate = 3e-5\n\n    # wrap tensors\n    train_data = TensorDataset(train_seq, train_mask, train_y)\n    # sampler for sampling the data during training\n    train_sampler = RandomSampler(train_data)\n    # dataLoader for train set\n    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n    # wrap tensors\n    val_data = TensorDataset(val_seq, val_mask, val_y)\n    # sampler for sampling the data during training\n    val_sampler = SequentialSampler(val_data)\n    # dataLoader for validation set\n    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n\n\n    # import BERT-base pretrained model\n    bert = BertModel.from_pretrained('bert-base-uncased')\n\n\n    # freeze all the parameters\n    for param in bert.parameters():\n        param.requires_grad = False\n\n\n    # pass the pre-trained BERT to our define architecture\n    model = BERT_LSTM_Model(bert, len(np.unique(labels)))\n    # push the model to GPU\n    model = model.to(device)\n\n\n\n    # define the optimizer\n    # optimizer = AdamW(model.parameters(), lr=2e-5)\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n\n\n    # set initial loss to infinite\n    best_valid_loss = float('inf')\n\n\n    epochs = 15\n    current = 1\n    # for each epoch\n    while current <= epochs:\n\n        print(f'\\nEpoch {current} / {epochs}:')\n\n        # train model\n        train_loss, _ = train(model,optimizer,train_dataloader,batch_size)\n\n        # evaluate model\n        valid_loss, preds = evaluate(model,val_dataloader,batch_size)\n\n        # save the best model\n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save(model.state_dict(), f'model_{ty}.pth')\n\n\n        print(f'\\n\\nTraining Loss: {train_loss:.3f}')\n        print(f'Validation Loss: {valid_loss:.3f}')\n        \n        preds = np.argmax(preds, axis=1)\n        print(classification_report(val_y, preds))\n\n        current = current + 1\n\n    # get predictions for test data\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"_uuid":"77eecd2c-21ce-448c-9500-0401cd3006e7","_cell_guid":"7c2f309e-3ee5-4373-8e0d-14238e179267","collapsed":false,"execution":{"iopub.status.busy":"2024-05-04T09:27:06.509530Z","iopub.execute_input":"2024-05-04T09:27:06.509772Z","iopub.status.idle":"2024-05-04T09:27:06.524532Z","shell.execute_reply.started":"2024-05-04T09:27:06.509750Z","shell.execute_reply":"2024-05-04T09:27:06.523662Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelss('HS')","metadata":{"_uuid":"3f1f1cdd-5c8c-4c87-9b32-418a53187aca","_cell_guid":"611ecfe9-3993-465f-8dff-bbea0fb5ccf4","collapsed":false,"execution":{"iopub.status.busy":"2024-05-04T09:27:06.526990Z","iopub.execute_input":"2024-05-04T09:27:06.527305Z","iopub.status.idle":"2024-05-04T10:15:19.030988Z","shell.execute_reply.started":"2024-05-04T09:27:06.527275Z","shell.execute_reply":"2024-05-04T10:15:19.030114Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelss('AG')","metadata":{"_uuid":"48c335bd-5e38-44fe-9022-b80d9faa340f","_cell_guid":"5bdf3ce7-ce00-4136-916e-a54dbb562f3b","collapsed":false,"execution":{"iopub.status.busy":"2024-05-04T10:15:19.032241Z","iopub.execute_input":"2024-05-04T10:15:19.032579Z","iopub.status.idle":"2024-05-04T11:04:03.623925Z","shell.execute_reply.started":"2024-05-04T10:15:19.032551Z","shell.execute_reply":"2024-05-04T11:04:03.622937Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelss('TR')","metadata":{"_uuid":"bb135929-0680-4ed0-984f-fc6ce621decc","_cell_guid":"a0e86aa6-598e-4fb3-a679-b335cd0ee9af","collapsed":false,"execution":{"iopub.status.busy":"2024-05-04T11:04:03.625675Z","iopub.execute_input":"2024-05-04T11:04:03.626088Z","iopub.status.idle":"2024-05-04T11:51:24.885618Z","shell.execute_reply.started":"2024-05-04T11:04:03.626053Z","shell.execute_reply":"2024-05-04T11:51:24.884808Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load test data\ntestdf = pd.read_csv('/kaggle/input/nlpdataset/test.csv')\ntest_data = pre_process_dataset(testdf['text'].tolist())\n\n# Load BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert = AutoModel.from_pretrained('bert-base-uncased')\n\n# Initialize an empty DataFrame to store predictions\npredictions_df = pd.DataFrame({'id': testdf['id']})\n\n# Define the labels\nlabels = ['HS', 'TR', 'AG']\n\n# Iterate over each label\nfor label in labels:\n    # Load the corresponding model\n    model_path = f'model_{label}.pth'\n    model = BERT_LSTM_Model(bert, 2)\n    model.load_state_dict(torch.load(model_path))\n    model = model.to(device)\n    \n    # Tokenize the test data\n    test_seq = torch.tensor([tokenizer.encode(i, max_length=36, pad_to_max_length=True) for i in test_data])\n    test_mask = torch.tensor([[float(i > 0) for i in ii] for ii in test_seq])\n    \n    # Get predictions for test data\n    with torch.no_grad():\n        preds = model(test_seq.to(device), test_mask.to(device))\n        preds = preds.detach().cpu().numpy()\n    \n    # Get the predicted labels\n    predictions_df[label] = np.argmax(preds, axis=1)\n\n# Apply conditions to the predictions\npredictions_df['TR'] = predictions_df.apply(lambda row: row['TR'] if row['HS'] == 1 else 0, axis=1)\npredictions_df['AG'] = predictions_df.apply(lambda row: row['AG'] if row['HS'] == 1 else 0, axis=1)\n\n# Round the predictions to binary values (0 or 1)\npredictions_df = predictions_df.round().astype(int)\n\n# Save the predictions to a CSV file\npredictions_df.to_csv('prediction_2.csv', index=False)","metadata":{"_uuid":"34acd085-8c10-4f8a-b24d-b251a4bb45c6","_cell_guid":"389f6851-08a9-4eaf-b161-4896034064a4","collapsed":false,"execution":{"iopub.status.busy":"2024-05-04T11:51:24.886766Z","iopub.execute_input":"2024-05-04T11:51:24.887058Z","iopub.status.idle":"2024-05-04T11:51:41.404354Z","shell.execute_reply.started":"2024-05-04T11:51:24.887033Z","shell.execute_reply":"2024-05-04T11:51:41.403593Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"a23b21fe-28fa-48cf-b58e-50301ae5f212","_cell_guid":"eaa72c01-af78-459f-8649-6283d951d9db","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}